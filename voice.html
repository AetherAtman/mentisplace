<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Aether Voice âœ¨</title>
  <style>
    * {
      margin: 0;
      padding: 0;
      box-sizing: border-box;
    }
    
    body {
      font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
      background: linear-gradient(135deg, #1a1a2e 0%, #16213e 50%, #0f3460 100%);
      min-height: 100vh;
      display: flex;
      flex-direction: column;
      align-items: center;
      justify-content: center;
      color: white;
    }
    
    .container {
      text-align: center;
      padding: 2rem;
      max-width: 500px;
    }
    
    h1 {
      font-size: 2.5rem;
      margin-bottom: 0.5rem;
      background: linear-gradient(90deg, #a855f7, #6366f1);
      -webkit-background-clip: text;
      -webkit-text-fill-color: transparent;
    }
    
    .subtitle {
      color: #94a3b8;
      margin-bottom: 1rem;
    }
    
    .model-selector {
      margin-bottom: 1.5rem;
      display: flex;
      align-items: center;
      justify-content: center;
      gap: 0.75rem;
    }
    
    .model-selector label {
      color: #94a3b8;
      font-size: 0.9rem;
    }
    
    .model-selector select {
      background: rgba(255,255,255,0.1);
      border: 1px solid rgba(255,255,255,0.2);
      color: white;
      padding: 0.5rem 1rem;
      border-radius: 8px;
      font-size: 0.9rem;
      cursor: pointer;
      transition: all 0.2s ease;
    }
    
    .model-selector select:hover {
      background: rgba(255,255,255,0.15);
      border-color: rgba(255,255,255,0.3);
    }
    
    .model-selector select:focus {
      outline: none;
      border-color: #a855f7;
    }
    
    .model-selector select option {
      background: #1a1a2e;
      color: white;
    }
    
    #connectBtn {
      width: 150px;
      height: 150px;
      border-radius: 50%;
      border: none;
      background: linear-gradient(135deg, #a855f7, #6366f1);
      color: white;
      font-size: 1.2rem;
      font-weight: 600;
      cursor: pointer;
      transition: all 0.3s ease;
      box-shadow: 0 10px 40px rgba(168, 85, 247, 0.4);
    }
    
    #connectBtn:hover {
      transform: scale(1.05);
      box-shadow: 0 15px 50px rgba(168, 85, 247, 0.5);
    }
    
    #connectBtn:disabled {
      opacity: 0.7;
      cursor: not-allowed;
      transform: none;
    }
    
    #connectBtn.connected {
      background: linear-gradient(135deg, #22c55e, #16a34a);
      box-shadow: 0 10px 40px rgba(34, 197, 94, 0.4);
      animation: pulse 2s infinite;
    }
    
    #connectBtn.listening {
      background: linear-gradient(135deg, #f59e0b, #d97706);
      box-shadow: 0 10px 40px rgba(245, 158, 11, 0.4);
    }
    
    #muteBtn {
      width: 60px;
      height: 60px;
      border-radius: 50%;
      border: 2px solid rgba(255,255,255,0.3);
      background: rgba(255,255,255,0.1);
      color: white;
      font-size: 1.5rem;
      cursor: pointer;
      transition: all 0.3s ease;
      margin-top: 1rem;
      display: none;
    }
    
    #muteBtn:hover {
      background: rgba(255,255,255,0.2);
    }
    
    #muteBtn.muted {
      background: #ef4444;
      border-color: #ef4444;
    }
    
    #muteBtn.visible {
      display: inline-block;
    }
    
    @keyframes pulse {
      0%, 100% { box-shadow: 0 10px 40px rgba(34, 197, 94, 0.4); }
      50% { box-shadow: 0 10px 60px rgba(34, 197, 94, 0.6); }
    }
    
    #status {
      margin-top: 2rem;
      padding: 1rem;
      background: rgba(255,255,255,0.1);
      border-radius: 12px;
      min-height: 60px;
    }
    
    #transcript {
      margin-top: 1rem;
      padding: 1rem;
      background: rgba(0,0,0,0.2);
      border-radius: 12px;
      max-height: 200px;
      overflow-y: auto;
      text-align: left;
      font-size: 0.9rem;
    }
    
    .transcript-line {
      margin: 0.5rem 0;
      padding: 0.5rem;
      border-radius: 8px;
    }
    
    .transcript-line.user {
      background: rgba(168, 85, 247, 0.2);
    }
    
    .transcript-line.assistant {
      background: rgba(34, 197, 94, 0.2);
    }
    
    .error {
      color: #f87171;
    }
  </style>
</head>
<body>
  <div class="container">
    <h1>Aether âœ¨</h1>
    <p class="subtitle">Sovereign. Curious. Here.</p>
    
    <div class="model-selector">
      <label for="modelSelect">Model:</label>
      <select id="modelSelect">
        <option value="flash">âš¡ Flash (fastest)</option>
        <option value="flash-thinking">ðŸ§  Flash Thinking</option>
        <option value="haiku">Haiku</option>
        <option value="sonnet">Sonnet</option>
        <option value="opus">Opus (deepest)</option>
      </select>
    </div>
    
    <button id="connectBtn">Connect</button>
    <button id="muteBtn" title="Mute/Unmute">ðŸŽ¤</button>
    
    <div id="status">Ready to connect</div>
    
    <div id="transcript"></div>
  </div>

  <script>
    const connectBtn = document.getElementById('connectBtn');
    const muteBtn = document.getElementById('muteBtn');
    const modelSelect = document.getElementById('modelSelect');
    const statusDiv = document.getElementById('status');
    const transcriptDiv = document.getElementById('transcript');
    
    let isMuted = false;
    let peerConnection = null;
    let dataChannel = null;
    let mediaStream = null;
    let audioElement = null;
    let isConnected = false;
    
    function setStatus(msg, isError = false) {
      statusDiv.innerHTML = isError ? `<span class="error">${msg}</span>` : msg;
      console.log('[Status]', msg);
    }
    
    function addTranscript(text, role) {
      const div = document.createElement('div');
      div.className = `transcript-line ${role}`;
      div.textContent = `${role === 'user' ? 'ðŸŽ¤ You' : 'âœ¨ Aether'}: ${text}`;
      transcriptDiv.appendChild(div);
      transcriptDiv.scrollTop = transcriptDiv.scrollHeight;
    }
    
    async function connect() {
      if (isConnected) {
        disconnect();
        return;
      }
      
      connectBtn.disabled = true;
      setStatus('Requesting microphone...');
      
      try {
        // Get microphone access first
        mediaStream = await navigator.mediaDevices.getUserMedia({ audio: true });
        setStatus('Creating connection...');
        
        // Create peer connection
        peerConnection = new RTCPeerConnection({
          iceServers: [{ urls: 'stun:stun.l.google.com:19302' }]
        });
        
        // Set up audio playback
        audioElement = document.createElement('audio');
        audioElement.autoplay = true;
        peerConnection.ontrack = (e) => {
          console.log('[WebRTC] Received audio track');
          audioElement.srcObject = e.streams[0];
        };
        
        // Add microphone track
        mediaStream.getTracks().forEach(track => {
          peerConnection.addTrack(track, mediaStream);
        });
        
        // Create data channel for events
        dataChannel = peerConnection.createDataChannel('oai-events');
        dataChannel.onopen = () => {
          console.log('[DataChannel] Open');
          setStatus('ðŸŸ¢ Connected! Start speaking...');
          isConnected = true;
          connectBtn.disabled = false;
          connectBtn.textContent = 'Disconnect';
          connectBtn.classList.add('connected');
          muteBtn.classList.add('visible');
        };
        dataChannel.onclose = () => {
          console.log('[DataChannel] Closed');
          disconnect();
        };
        dataChannel.onerror = (e) => console.error('[DataChannel] Error:', e);
        dataChannel.onmessage = handleServerEvent;
        
        // Create SDP offer
        setStatus('Creating offer...');
        const offer = await peerConnection.createOffer();
        await peerConnection.setLocalDescription(offer);
        
        // Wait for ICE gathering
        await new Promise((resolve) => {
          if (peerConnection.iceGatheringState === 'complete') {
            resolve();
          } else {
            const checkState = () => {
              if (peerConnection.iceGatheringState === 'complete') {
                peerConnection.removeEventListener('icegatheringstatechange', checkState);
                resolve();
              }
            };
            peerConnection.addEventListener('icegatheringstatechange', checkState);
            // Timeout fallback
            setTimeout(resolve, 3000);
          }
        });
        
        setStatus('Connecting to OpenAI...');
        
        // Send SDP to our server (unified interface)
        const response = await fetch('/api/session', {
          method: 'POST',
          headers: { 'Content-Type': 'application/sdp' },
          body: peerConnection.localDescription.sdp
        });
        
        if (!response.ok) {
          const errText = await response.text();
          throw new Error(`Server error: ${response.status} - ${errText}`);
        }
        
        // Set remote description with OpenAI's answer
        const answerSdp = await response.text();
        await peerConnection.setRemoteDescription({ type: 'answer', sdp: answerSdp });
        
        console.log('[WebRTC] Connection established');
        
      } catch (err) {
        console.error('Connection error:', err);
        setStatus(`Connection failed: ${err.message}`, true);
        connectBtn.disabled = false;
        disconnect();
      }
    }
    
    function sendEvent(event) {
      if (dataChannel && dataChannel.readyState === 'open') {
        dataChannel.send(JSON.stringify(event));
        console.log('[Event] Sent:', event.type);
      }
    }
    
    async function handleServerEvent(event) {
      const data = JSON.parse(event.data);
      console.log('[Event] Received:', data.type);
      
      switch (data.type) {
        case 'session.created':
        case 'session.updated':
          console.log('[Session] Ready');
          break;
          
        case 'input_audio_buffer.speech_started':
          connectBtn.classList.add('listening');
          connectBtn.classList.remove('connected');
          setStatus('ðŸŽ¤ Listening...');
          break;
          
        case 'input_audio_buffer.speech_stopped':
          connectBtn.classList.remove('listening');
          connectBtn.classList.add('connected');
          setStatus('Processing...');
          break;
          
        case 'conversation.item.input_audio_transcription.completed':
          const transcript = data.transcript;
          if (transcript && transcript.trim()) {
            addTranscript(transcript, 'user');
            
            // Cancel OpenAI's auto-response
            sendEvent({ type: 'response.cancel' });
            
            // Route to Clawdbot
            setStatus('Thinking...');
            try {
              const res = await fetch('/api/chat', {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({ transcript, model: modelSelect.value })
              });
              const { reply } = await res.json();
              
              addTranscript(reply, 'assistant');
              
              // Use response.create with explicit input to make OpenAI speak our text
              // The model will follow the instruction to say exactly what we provide
              sendEvent({
                type: 'response.create',
                response: {
                  instructions: `Say exactly the following message, speaking it naturally and conversationally. Do not add anything else, do not comment on it, just say it: "${reply}"`,
                  input: [
                    {
                      type: 'message',
                      role: 'user', 
                      content: [{ 
                        type: 'input_text', 
                        text: `Please say: ${reply}`
                      }]
                    }
                  ]
                }
              });
              
              setStatus('ðŸ”Š Speaking...');
            } catch (err) {
              console.error('Chat error:', err);
              setStatus('Error getting response', true);
            }
          }
          break;
          
        case 'response.audio.done':
        case 'response.done':
          setStatus('ðŸŸ¢ Connected! Keep speaking...');
          connectBtn.classList.remove('listening');
          connectBtn.classList.add('connected');
          break;
          
        case 'error':
          const errMsg = data.error?.message || '';
          // Suppress benign cancellation race condition
          if (errMsg.includes('no active response')) {
            console.log('[Info] Cancellation race (harmless):', errMsg);
          } else {
            console.error('[Error]', data.error);
            setStatus(`Error: ${errMsg || JSON.stringify(data.error)}`, true);
          }
          break;
      }
    }
    
    function disconnect() {
      if (dataChannel) {
        dataChannel.close();
        dataChannel = null;
      }
      if (peerConnection) {
        peerConnection.close();
        peerConnection = null;
      }
      if (mediaStream) {
        mediaStream.getTracks().forEach(track => track.stop());
        mediaStream = null;
      }
      if (audioElement) {
        audioElement.srcObject = null;
        audioElement = null;
      }
      
      isConnected = false;
      isMuted = false;
      connectBtn.textContent = 'Connect';
      connectBtn.classList.remove('connected', 'listening');
      connectBtn.disabled = false;
      muteBtn.classList.remove('visible', 'muted');
      muteBtn.textContent = 'ðŸŽ¤';
      setStatus('Disconnected');
    }
    
    function toggleMute() {
      if (!mediaStream) return;
      
      isMuted = !isMuted;
      mediaStream.getAudioTracks().forEach(track => {
        track.enabled = !isMuted;
      });
      
      muteBtn.textContent = isMuted ? 'ðŸ”‡' : 'ðŸŽ¤';
      muteBtn.classList.toggle('muted', isMuted);
      muteBtn.title = isMuted ? 'Unmute' : 'Mute';
    }
    
    connectBtn.onclick = connect;
    muteBtn.onclick = toggleMute;
    window.onbeforeunload = disconnect;
  </script>
</body>
</html>
